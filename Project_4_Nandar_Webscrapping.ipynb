{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 4: Web Scraping Job Postings\n",
    "\n",
    "## Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal has two main objectives:\n",
    "\n",
    "   1. Determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "   2. Determine the factors that distinguish job categories and titles from each other. For example, can required skills accurately predict job title?\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.*\n",
    "\n",
    "Hint: Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries. \n",
    "\n",
    "**Goal:** Scrape your own data from a job aggregation tool like Indeed.com in order to collect the data to best answer these two questions.\n",
    "\n",
    "---\n",
    "\n",
    "## Directions\n",
    "\n",
    "In this project you will be leveraging a variety of skills. The first will be to use the web-scraping and/or API techniques you've learned to collect data on data jobs from Indeed.com or another aggregator. Once you have collected and cleaned the data, you will use it to answer the two questions described above.\n",
    "\n",
    "### QUESTION 1: Factors that impact salary\n",
    "\n",
    "To predict salary you will be building either a classification or regression model, using features like the location, title, and summary of the job. If framing this as a regression problem, you will be estimating the listed salary amounts. You may instead choose to frame this as a classification problem, in which case you will create labels from these salaries (high vs. low salary, for example) according to thresholds (such as median salary).\n",
    "\n",
    "You have learned a variety of new skills and models that may be useful for this problem:\n",
    "- NLP\n",
    "- Unsupervised learning and dimensionality reduction techniques (PCA, clustering)\n",
    "- Ensemble methods and decision tree models\n",
    "- SVM models\n",
    "\n",
    "Whatever you decide to use, the most important thing is to justify your choices and interpret your results. *Communication of your process is key.* Note that most listings **DO NOT** come with salary information. You'll need to able to extrapolate or predict the expected salaries for these listings.\n",
    "\n",
    "### QUESTION 2: Factors that distinguish job category\n",
    "\n",
    "Using the job postings you scraped for part 1 (or potentially new job postings from a second round of scraping), identify features in the data related to job postings that can distinguish job titles from each other. There are a variety of interesting ways you can frame the target variable, for example:\n",
    "- What components of a job posting distinguish data scientists from other data jobs?\n",
    "- What features are important for distinguishing junior vs. senior positions?\n",
    "- Do the requirements for titles vary significantly with industry (e.g. healthcare vs. government)?\n",
    "\n",
    "You may end up making multiple classification models to tackle different questions. Be sure to clearly explain your hypotheses and framing, any feature engineering, and what your target variables are. The type of classification model you choose is up to you. Be sure to interpret your results and evaluate your models' performance.\n",
    "\n",
    "\n",
    "### BONUS PROBLEM\n",
    "\n",
    "Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your models to ease his mind, and explain what it is doing and any tradeoffs. Plot the ROC curve.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Scrape and prepare your own data.\n",
    "\n",
    "2. **Create and compare at least two models for each section**. One of the two models should be a decision tree or ensemble model. The other can be a classifier or regression of your choosing (e.g. Ridge, logistic regression, KNN, SVM, etc).\n",
    "   - Section 1: Job Salary Trends\n",
    "   - Section 2: Job Category Factors\n",
    "\n",
    "3. Prepare a polished Jupyter Notebook with your analysis for a peer audience of data scientists. \n",
    "   - Make sure to clearly describe and label each section.\n",
    "   - Comment on your code so that others could, in theory, replicate your work.\n",
    "\n",
    "4. A brief writeup in an executive summary, written for a non-technical audience.\n",
    "   - Writeups should be at least 500-1000 words, defining any technical terms, explaining your approach, as well as any risks and limitations.\n",
    "\n",
    "#### BONUS\n",
    "\n",
    "5. Answer the salary discussion by using your model to explain the tradeoffs between detecting high vs low salary positions.\n",
    "\n",
    "6. Convert your executive summary into a public blog post of at least 500 words, in which you document your approach in a tutorial for other aspiring data scientists. Link to this in your notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Suggestions for Getting Started\n",
    "\n",
    "1. Collect data from [Indeed.com](www.indeed.com) (or another aggregator) on data-related jobs to use in predicting salary trends for your analysis.\n",
    "  - Select and parse data from *at least 1000 postings* for jobs, potentially from multiple location searches.\n",
    "2. Find out what factors most directly impact salaries (e.g. title, location, department, etc).\n",
    "  - Test, validate, and describe your models. What factors predict salary category? How do your models perform?\n",
    "3. Discover which features have the greatest importance when determining a low vs. high paying job.\n",
    "  - Your Boss is interested in what overall features hold the greatest significance.\n",
    "  - HR is interested in which SKILLS and KEY WORDS hold the greatest significance.   \n",
    "4. Author an executive summary that details the highlights of your analysis for a non-technical audience.\n",
    "5. If tackling the bonus question, try framing the salary problem as a classification problem detecting low vs. high salary positions.\n",
    "\n",
    "---\n",
    "\n",
    "## Useful Resources\n",
    "\n",
    "- Scraping is one of the most fun, useful and interesting skills out there. Don’t lose out by copying someone else's code!\n",
    "- [Here is some advice on how to write for a non-technical audience](http://programmers.stackexchange.com/questions/11523/explaining-technical-things-to-non-technical-people)\n",
    "- [Documentation for BeautifulSoup can be found here](http://www.crummy.com/software/BeautifulSoup/).\n",
    "\n",
    "---\n",
    "\n",
    "### Project Feedback + Evaluation\n",
    "\n",
    "For all projects, students will be evaluated on a simple 3 point scale (0, 1, or 2). Instructors will use this rubric when scoring student performance on each of the core project **requirements:** \n",
    "\n",
    "Score | Expectations\n",
    "----- | ------------\n",
    "**0** | _Does not meet expectations. Try again._\n",
    "**1** | _Meets expectations. Good job._\n",
    "**2** | _Surpasses expectations. Brilliant!_\n",
    "\n",
    "[For more information on how we grade our DSI projects, see our project grading walkthrough.](https://git.generalassemb.ly/dsi-projects/readme/blob/master/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Save all the job links to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping job links is starting at  2019-05-07 23:25:02.622192\n",
      "Page 0 is completed.....\n",
      "Page 10 is completed.....\n",
      "Page 20 is completed.....\n",
      "Page 30 is completed.....\n",
      "Page 40 is completed.....\n",
      "Page 50 is completed.....\n",
      "Page 60 is completed.....\n",
      "Page 70 is completed.....\n",
      "Page 80 is completed.....\n",
      "Page 90 is completed.....\n",
      "Page 100 is completed.....\n",
      "Page 110 is completed.....\n",
      "Page 120 is completed.....\n",
      "Page 130 is completed.....\n",
      "Page 140 is completed.....\n",
      "Page 150 is completed.....\n",
      "Page 160 is completed.....\n",
      "Page 170 is completed.....\n",
      "Page 180 is completed.....\n",
      "Page 190 is completed.....\n",
      "Page 200 is completed.....\n",
      "Page 210 is completed.....\n",
      "Page 220 is completed.....\n",
      "Page 230 is completed.....\n",
      "236 is the last page to scrape....\n",
      "The file does not exist\n",
      "Scrapping job links is ending at  2019-05-07 23:53:11.531912\n",
      "Total operation hours are 0 hours 28.133333333333333 minutes\n"
     ]
    }
   ],
   "source": [
    "job_links_starting_time = datetime.datetime.now()\n",
    "print(\"Scrapping job links is starting at \",job_links_starting_time)\n",
    "\n",
    "search_word = \"data\"\n",
    "links = []\n",
    "driver = webdriver.Chrome(executable_path='./chromedriver/chromedriver')\n",
    "\n",
    "for page_no in range(0,300):\n",
    "    #print(\"Page no {}\".format(page_no))\n",
    "    driver.get(\"https://www.mycareersfuture.sg/search?search={}&sortBy=new_posting_date&page={}\".format(search_word,page_no))\n",
    " \n",
    "    sleep(random.randint(4,8))\n",
    "    assert \"MyCareersFuture\" in driver.title\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    all_elements = soup.find_all(\"a\",{\"class\": \"bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3\"})\n",
    "    for each_entry in all_elements:\n",
    "        links.append(\"https://www.mycareersfuture.sg\"+each_entry.get('href'))\n",
    "\n",
    "    next_page_signs = soup.find_all(\"span\",{\"type\":\"action\"})\n",
    "    page_signs = [i.get_text() for i in next_page_signs]\n",
    "            \n",
    "    if page_no%10 == 0:\n",
    "        print(\"Page {} is completed.....\".format(page_no))\n",
    "        link_df = pd.DataFrame({'urls':links})\n",
    "        link_df.to_csv(\"./datasets/MyCareersFuture_Job_Links_Up_to_page_{}.csv\".format(page_no),index=False)\n",
    "       \n",
    "    if '❯' not in page_signs:\n",
    "        print(\"{} is the last page to scrape....\".format(page_no))\n",
    "        break\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Create the dataframe for urls and save it to csv file so that no need to run again.\n",
    "link_table = pd.DataFrame({'No':range(1,len(links)+1),'urls':links})\n",
    "link_table.to_csv(\"./datasets/MyCareersFuture_All_Job_Links.csv\",index=False)\n",
    "\n",
    "# Deleting the backup csv files which are created during scrapping process.\n",
    "for i in range(0,page_no+10,10):\n",
    "    if os.path.exists(\"./datasets/MyCareersFuture_Job_Links_Up_to_page_{}.csv\".format(i)):\n",
    "        os.remove(\"./datasets/MyCareersFuture_Job_Links_Up_to_page_{}.csv\".format(i))\n",
    "    else:\n",
    "        print(\"The file does not exist\")\n",
    "\n",
    "job_links_ending_time = datetime.datetime.now()\n",
    "print(\"Scrapping job links is ending at \",job_links_ending_time)\n",
    "diff = job_links_ending_time - job_links_starting_time\n",
    "print(\"Total operation hours are\",int(diff.seconds/(60*60)),\"hours\",\n",
    "      (diff.seconds%(60*60))/60,\"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736\n"
     ]
    }
   ],
   "source": [
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inspecting the links that we scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before checking duplicates :  (4736, 2)\n",
      "After checking duplicates :  (4736, 2)\n",
      "   No                                               urls\n",
      "0   1  https://www.mycareersfuture.sg/job/data-scient...\n",
      "1   2  https://www.mycareersfuture.sg/job/data-center...\n",
      "2   3  https://www.mycareersfuture.sg/job/data-engine...\n",
      "3   4  https://www.mycareersfuture.sg/job/field-opera...\n",
      "4   5  https://www.mycareersfuture.sg/job/senior-data...\n",
      "        No                                               urls\n",
      "4731  4732  https://www.mycareersfuture.sg/job/principal-m...\n",
      "4732  4733  https://www.mycareersfuture.sg/job/deputy-dire...\n",
      "4733  4734  https://www.mycareersfuture.sg/job/mha-science...\n",
      "4734  4735  https://www.mycareersfuture.sg/job/business-an...\n",
      "4735  4736  https://www.mycareersfuture.sg/job/business-an...\n",
      "\n",
      "First link :  https://www.mycareersfuture.sg/job/data-scientist-panasonic-asia-pacific-a19e1b45f8108c2367d59174ef3b1986\n"
     ]
    }
   ],
   "source": [
    "link_df = pd.read_csv(\"./datasets/MyCareersFuture_All_Job_Links.csv\")\n",
    "print(\"Before checking duplicates : \",link_df.shape)\n",
    "link_df.urls.duplicated(keep='first')\n",
    "print(\"After checking duplicates : \",link_df.shape)\n",
    "print(link_df.head())\n",
    "print(link_df.tail())\n",
    "print(\"\\nFirst link : \",link_df.urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Scrapping each job link to get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(columns=['url','id','title','company','location',\n",
    "                                'employment_type','seniority','min_experience',\n",
    "                                'salary_range','salary_type','no_applicants',\n",
    "                                'posted_date','closing_date','description','requirement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url,html,row):\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'url'] = url\n",
    "    except:\n",
    "        jobs_df.loc[row,'url'] = \"None\"\n",
    "        \n",
    "    try:\n",
    "        jobs_df.loc[row,'id'] = soup.find('span',{'class':'black-60 db f6 fw4 mv1'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'id'] = \"None\"\n",
    "    \n",
    "    try:\n",
    "        jobs_df.loc[row,'title'] = soup.find('h1',{'id':'job_title'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'title'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'company'] = soup.find('p',{'name':'company'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'company'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'location'] = soup.find('p',{'id':'address'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'location'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'employment_type'] = soup.find('p',{'id':'employment_type'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'employment_type'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'seniority'] = soup.find('p',{'id':'seniority'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'seniority'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'min_experience'] = soup.find('p',{'id':'min_experience'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'min_experience'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'salary_range'] = soup.find('div',{'class':'lh-solid'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'salary_range'] = \"None\"\n",
    "        \n",
    "    try:\n",
    "        jobs_df.loc[row,'salary_type'] = soup.find('span',{'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'salary_type'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'no_applicants'] = soup.find('span',{'id':'num_of_applications'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'no_applicants'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'posted_date'] = soup.find('span',{'id':'last_posted_date'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'posted_date'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'closing_date'] = soup.find('span',{'id':'expiry_date'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'closing_date'] = \"None\"\n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'description'] = soup.find('div',{'id':'description-content'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'description'] = \"None\"  \n",
    "\n",
    "    try:\n",
    "        jobs_df.loc[row,'requirement'] = soup.find('div',{'id':'requirements-content'}).get_text()\n",
    "    except:\n",
    "        jobs_df.loc[row,'requirement'] = \"None\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping all jobs info is starting at  2019-05-07 23:53:11.602539\n",
      "0 jobs are completed.....\n",
      "100 jobs are completed.....\n",
      "200 jobs are completed.....\n",
      "300 jobs are completed.....\n",
      "400 jobs are completed.....\n",
      "500 jobs are completed.....\n",
      "600 jobs are completed.....\n",
      "700 jobs are completed.....\n",
      "800 jobs are completed.....\n",
      "900 jobs are completed.....\n",
      "1000 jobs are completed.....\n",
      "1100 jobs are completed.....\n",
      "1200 jobs are completed.....\n",
      "1300 jobs are completed.....\n",
      "1400 jobs are completed.....\n",
      "1500 jobs are completed.....\n",
      "1600 jobs are completed.....\n",
      "1700 jobs are completed.....\n",
      "1800 jobs are completed.....\n",
      "1900 jobs are completed.....\n",
      "2000 jobs are completed.....\n",
      "2100 jobs are completed.....\n",
      "2200 jobs are completed.....\n",
      "2300 jobs are completed.....\n",
      "2400 jobs are completed.....\n",
      "2500 jobs are completed.....\n",
      "2600 jobs are completed.....\n",
      "2700 jobs are completed.....\n",
      "2800 jobs are completed.....\n",
      "2900 jobs are completed.....\n",
      "3000 jobs are completed.....\n",
      "3100 jobs are completed.....\n",
      "3200 jobs are completed.....\n",
      "3300 jobs are completed.....\n",
      "3400 jobs are completed.....\n",
      "3500 jobs are completed.....\n",
      "3600 jobs are completed.....\n",
      "3700 jobs are completed.....\n",
      "3800 jobs are completed.....\n",
      "3900 jobs are completed.....\n",
      "4000 jobs are completed.....\n",
      "4100 jobs are completed.....\n",
      "4200 jobs are completed.....\n",
      "4300 jobs are completed.....\n",
      "4400 jobs are completed.....\n",
      "4500 jobs are completed.....\n",
      "4600 jobs are completed.....\n",
      "4700 jobs are completed.....\n",
      "The file does not exist\n",
      "Scrapping Operation is completed successfully.... Total 4735 jobs are scrapped.\n",
      "Operation is ended at  2019-05-07 23:53:11.602539\n",
      "Total operation hours are 2 hours 39 minutes\n"
     ]
    }
   ],
   "source": [
    "job_info_starting_time = datetime.datetime.now()\n",
    "print(\"Scrapping all jobs info is starting at \",job_info_starting_time)\n",
    "\n",
    "driver1 = webdriver.Chrome(executable_path='./chromedriver/chromedriver')\n",
    "driver2 = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "driver3 = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "driver4 = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "driver5 = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "drivers = [driver1,driver2,driver3,driver4,driver5]\n",
    "#print(\"Accessing {} .....\".format(link_df.urls[0]))\n",
    "#for i in range(4500,link_df.shape[0],5):   # for testing\n",
    "for i in range(0,link_df.shape[0]-4,5):\n",
    "    driver1.get(link_df.urls[i])\n",
    "    driver2.get(link_df.urls[i+1])\n",
    "    driver3.get(link_df.urls[i+2])\n",
    "    driver4.get(link_df.urls[i+3])\n",
    "    driver5.get(link_df.urls[i+4])\n",
    "\n",
    "#     print(driver1.title)\n",
    "#     print(driver2.title)\n",
    "#     print(driver3.title)\n",
    "#     print(driver4.title)\n",
    "#     print(driver5.title)\n",
    "#     assert \"MyCareersFuture\" in driver1.title\n",
    "#     assert \"MyCareersFuture\" in driver2.title\n",
    "#     assert \"MyCareersFuture\" in driver3.title\n",
    "#     assert \"MyCareersFuture\" in driver4.title\n",
    "#     assert \"MyCareersFuture\" in driver5.title\n",
    "    \n",
    "    sleep(random.randint(4,8))\n",
    "    \n",
    "    for driver in drivers:\n",
    "        html = driver.page_source\n",
    "        get_soup(link_df.urls[i+drivers.index(driver)],html,i+drivers.index(driver))\n",
    "    \n",
    "    #if i%10 == 0:   # for testing\n",
    "    if i%100 == 0:\n",
    "        print(\"{} jobs are completed.....\".format(i))\n",
    "        jobs_df.to_csv(\"./datasets/MyCareersFuture_Scrapping_Up_to_Jobs_{}.csv\".format(i),index=False)\n",
    "    \n",
    "driver1.close()\n",
    "driver2.close()\n",
    "driver3.close()\n",
    "driver4.close()\n",
    "driver5.close()\n",
    "\n",
    "jobs_df.to_csv(\"./datasets/MyCareersFuture_All_Jobs.csv\",index=False)\n",
    "\n",
    "#for j in range(4500,i+10,10):   # for testing\n",
    "for j in range(0,i+100,100):\n",
    "    if os.path.exists(\"./datasets/MyCareersFuture_Scrapping_Up_to_Jobs_{}.csv\".format(j)):\n",
    "        os.remove(\"./datasets/MyCareersFuture_Scrapping_Up_to_Jobs_{}.csv\".format(j))\n",
    "    else:\n",
    "        print(\"The file does not exist\")\n",
    "        \n",
    "print(\"Scrapping Operation is completed successfully.... Total {} jobs are scrapped.\".format(i+5))\n",
    "job_info_ending_time = datetime.datetime.now()\n",
    "print(\"Operation is ended at \",job_info_starting_time)\n",
    "time_diff = job_info_ending_time - job_info_starting_time\n",
    "print(\"Total operation hours are\",int(time_diff.seconds/(60*60)),\"hours\",\n",
    "      int((time_diff.seconds%(60*60))/60),\"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
